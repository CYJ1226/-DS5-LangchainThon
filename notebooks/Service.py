# API ë¶ˆëŸ¬ì˜¤ê¸°
import os
from dotenv import load_dotenv

# ì›ë³¸ íŒŒì¼ ì •ë¦¬
import pandas as pd
from langchain_community.document_loaders import PyPDFLoader, UnstructuredHTMLLoader, Docx2txtLoader
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain_core.documents import Document

import datetime as dt
from zoneinfo import ZoneInfo
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.retrievers import ContextualCompressionRetriever
from langchain_cohere import CohereRerank
from langchain_community.document_transformers import LongContextReorder
from langchain.retrievers.document_compressors import DocumentCompressorPipeline

# streamlit
import streamlit as st
from streamlit_lottie import st_lottie
import requests


#ì˜¤í”ˆAI API í‚¤ ì„¤ì •
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
COHERE_API_KEY = os.getenv("COHERE_API_KEY")

# ============================================== #

# Embedding - Chunk split
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=128,
    separators=["\n\n", "\n", " ", ""]             
)

### pdf íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
PDF_PATH = r"../data/Data/pdf/ëª¨ë‘ì—° ë¸Œëœë”©ë¶ ì •ë¦¬.pdf"

loader_pdf = PyPDFLoader(PDF_PATH)
pages_pdf = loader_pdf.load()

for d in pages_pdf:
    d.metadata["source_type"] = "pdf"
    d.metadata["source"] = os.path.basename(PDF_PATH) # basename: íŒŒì¼ ì´ë¦„ë§Œ ì¶”ì¶œ

docs_pdf = text_splitter.split_documents(pages_pdf)

### html íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
HTML_PATH = [
    r"../data/Data/html/LMS oops í•´ê²°ë²•.html",
    r"../data/Data/html/LMS ì•„ì´í  ë…¸íŠ¸ë¶ì´ ì•„ë‹™ë‹ˆë‹¤ ì—ëŸ¬.html",
    r"../data/Data/html/LMS ì´ìš©ì‹œ ë°œìƒí•˜ëŠ” ë¬¸ì œ í•´ê²°ë²•.html",
    r"../data/Data/html/êµìœ¡ê³¼ì • ì¤‘ ì·¨ì—… ì‹œ.html",
    r"../data/Data/html/ë°ì‹¸ 5ê¸° í›ˆë ¨ ì •ë³´.html",
    r"../data/Data/html/ìˆ˜ê°• ì¤‘ ê³ ìš© í˜•íƒœ ê´€ë ¨ ì•ˆë‚´.html",
    r"../data/Data/html/ìŠ¤í„°ë””ë¥¼ ë§Œë“¤ê³  ì‹¶ì€ë° ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”.html",
    r"../data/Data/html/ì˜¤í”„ë‹ ì¥ì†Œì™€ í´ë¡œì§• ì¥ì†Œê°€ ë‹¤ë¦…ë‹ˆë‹¤.html",
    r"../data/Data/html/ì¸í„°ë„·ì´ ë¶ˆì•ˆì •í•˜ì—¬ ì¶œê²° QRì„ ì œëŒ€ë¡œ ì°ì§€ ëª»í•˜ì˜€ìŠµë‹ˆë‹¤.html",
    r"../data/Data/html/ì œì  ê°€ì´ë“œ.html",
    r"../data/Data/html/ì¶œê²° ë° ê³µê°€ì— ëŒ€í•˜ì—¬.html",
    r"../data/Data/html/íˆ´ ì„¸íŒ….html",
    r"../data/Data/html/í›ˆë ¨ ì¥ë ¤ê¸ˆ ì§€ê¸‰ í™•ì¸.html",
    r"../data/Data/html/í›ˆë ¨ ì°¸ì—¬ ê·œì¹™.html",
]

html_list = []

# ê° íŒŒì¼ ë¡œë“œ + Metadata ì €ì¥
for path in HTML_PATH:
    loader_html = UnstructuredHTMLLoader(path)
    pages_html = loader_html.load()

    for d in pages_html:
        d.metadata["source_type"] = "html"
        d.metadata["source"] = os.path.basename(path)

    html_list.extend(pages_html)  

docs_html = text_splitter.split_documents(html_list)


### word íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°

WORD_PATH = r"../data/Data/word/íœ´ê°€ì‹ ì²­ì„œ(ë°ì‹¸_5ê¸°).docx"
loader_word = Docx2txtLoader(WORD_PATH)
pages_word = loader_word.load()

for d in pages_word:
    d.metadata["source_type"] = "word"
    d.metadata["source"] = os.path.basename(WORD_PATH)

docs_word = text_splitter.split_documents(pages_word)


### csv íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°

CSV_PATH = [
    r"../data/Data/csv/ë°ì‹¸ 5ê¸° ë™ë£Œë“¤.csv",
    r"../data/Data/csv/ë°ì‹¸ 5ê¸° ìš´ì˜ì§„.csv"
]

csv_list = []

# ê° íŒŒì¼ ë¡œë“œ
for path in CSV_PATH:
    loader_csv = CSVLoader(path, encoding = 'cp949')
    pages_csv = loader_csv.load()

    for d in pages_csv:
        d.metadata["source_type"] = "csv"
        d.metadata["source"] = os.path.basename(path)

    csv_list.extend(pages_csv)  

docs_csv = text_splitter.split_documents(csv_list)


### csv - calendar íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°

# í•™ìƒ ì´ë¦„ì„ ê¸°ì¤€ ê·¸ë£¹í™” í•¨ìˆ˜
def create_grouped_documents(csv_path: str) -> list[Document]:
    """
    CSV íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ í•™ìƒ ì´ë¦„ë³„ë¡œ ì¶œê²° ê¸°ë¡ì„ ê·¸ë£¹í™”í•˜ê³ 
    LangChain Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        csv_path: ì¶œê²° CSV íŒŒì¼ì˜ ê²½ë¡œ.

    Returns:
        Document ê°ì²´ ë¦¬ìŠ¤íŠ¸. ê° DocumentëŠ” í•œ í•™ìƒì˜ ì „ì²´ ê¸°ë¡ì„ ë‹´ìŠµë‹ˆë‹¤.
    """
    # 1. CSV íŒŒì¼ ë¡œë“œ
    df = pd.read_csv(csv_path, encoding='cp949')

    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ê³  NaN ê°’ì€ ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´ (ë¬¸ìì—´ ê²°í•© ì‹œ ì˜¤ë¥˜ ë°©ì§€)
    required_cols = ['ì´ë¦„', 'ì‚¬ìœ ', 'ë‚ ì§œ', 'ë¶€ì¬ì‹œê°„', 'ìƒíƒœ']
    if not all(col in df.columns for col in required_cols):
        print(f"ì˜¤ë¥˜: CSV íŒŒì¼ì— í•„ìš”í•œ ì»¬ëŸ¼ ({required_cols}) ì¤‘ ì¼ë¶€ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return []

    df = df[required_cols].fillna('')

    # Document ê°ì²´ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    documents = []

    # 2. 'ì´ë¦„' ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”
    grouped = df.groupby('ì´ë¦„')

    # 3. ê° í•™ìƒ ê·¸ë£¹ì„ í•˜ë‚˜ì˜ ê¸´ í…ìŠ¤íŠ¸ Documentë¡œ ë³€í™˜
    for name, group_df in grouped:
        # í•™ìƒë³„ ê¸°ë¡ì„ ë¬¸ìì—´ë¡œ ë³€í™˜ (ë‚ ì§œ, ë¶€ì¬ì‹œê°„, ìƒíƒœë§Œ í¬í•¨)
        # 'ì´ë¦„' ì»¬ëŸ¼ì€ ë©”íƒ€ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— í…ìŠ¤íŠ¸ ë‚´ìš©ì—ì„œëŠ” ì œì™¸í•©ë‹ˆë‹¤.
        record_strings = []
        for index, row in group_df.iterrows():
            record = (
                f"ì‚¬ìœ : {row['ì‚¬ìœ ']}, "
                f"ë‚ ì§œ: {row['ë‚ ì§œ']}, "
                f"ìƒíƒœ: {row['ìƒíƒœ']}, "
                f"ë¶€ì¬ì‹œê°„: {row['ë¶€ì¬ì‹œê°„']}"
            )
            record_strings.append(record)

        # ëª¨ë“  ê¸°ë¡ì„ ì¤„ ë°”ê¿ˆìœ¼ë¡œ ì—°ê²°í•˜ì—¬ í•˜ë‚˜ì˜ ê¸´ í…ìŠ¤íŠ¸ ìƒì„±
        full_records_text = "\n".join(record_strings)

        # ìµœì¢… Document ê°ì²´ ìƒì„±
        document = Document(
            page_content=(
                f"í•™ìƒ ì´ë¦„: {name}\n\n"
                f"--- ì „ì²´ ì¶œê²° ê¸°ë¡ ì‹œì‘ ---\n"
                f"{full_records_text}"
            ),
            # ë©”íƒ€ë°ì´í„°ì— í•µì‹¬ ì •ë³´ ì €ì¥ (ê²€ìƒ‰ ì‹œ í™œìš© ê°€ëŠ¥)
            metadata={'í•™ìƒì´ë¦„': name, 'ì´ê¸°ë¡ìˆ˜': len(group_df)}
        )
        documents.append(document)

    return documents

# í•™ìƒë³„ Document ë¦¬ìŠ¤íŠ¸ ìƒì„±
attendance_documents = create_grouped_documents(csv_path=r"../data/Data/csv/ë°ì‹¸ 5ê¸° ì¼ì •í‘œ.csv")
docs_attendance = text_splitter.split_documents(attendance_documents)


### VECTOR DB
# ChromaDB ë²¡í„° ì„ë² ë”© í›„ ì €ì¥
vectorstore = Chroma.from_documents(docs_html, OpenAIEmbeddings(model='text-embedding-3-large')) # Chroma.from_documents() ì½”ë“œê°€ ì‹¤í–‰ë˜ëŠ” ì‹œì ì— ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•œ í•µì‹¬ ì¤€ë¹„ ê³¼ì •ì´ ì™„ë£Œë©ë‹ˆë‹¤. ìœ ì‚¬ì„±ì„ ì¸¡ì •í•˜ëŠ” ë°©ì‹ ê·¸ ìì²´ë¥¼ ê²°ì •
vectorstore.add_documents(docs_word)
vectorstore.add_documents(docs_csv)
vectorstore.add_documents(docs_pdf)
vectorstore.add_documents(docs_attendance)


### RAG
# Reranking ì´ì „ base 
base_retriever = vectorstore.as_retriever( # ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì²­í¬ë¥¼ ì„ íƒí• ì§€ë¥¼ ê²°ì •
    search_type="mmr", # ì§ˆë¬¸ê³¼ì˜ ìœ ì‚¬ì„±+ì²­í¬ê°„ì˜ ë‹¤ì–‘ì„± => ì§ˆë¬¸ì´ ê´‘ë²”ìœ„í•˜ê³  ë‹µë³€ì´ ì¢…í•©ì ì´ì–´ì•¼ í•  ë•Œ ì‚¬ìš©
    search_kwargs={"lambda_mult": 0.4, "fetch_k": 96, "k": 48}
)

# Rerank
reranker = CohereRerank( # ê²€ìƒ‰ëœ ì²­í¬ì˜ ìˆœì„œë¥¼ ì¬ì •ë ¬ (ë…¸ì´ì¦ˆ ì œê±°)
    model="rerank-multilingual-v3.0",    
    top_n=10                              
)

# Reorder
reorder = LongContextReorder() # ì²­í¬ ìˆœì„œ ì¬ë°°ì—´(lost in the middle í•´ê²°)

# Rerank + Reorder
compressor = DocumentCompressorPipeline(transformers=[reranker, reorder])

upgraded_retriever = ContextualCompressionRetriever(
    base_retriever=base_retriever,
    base_compressor=compressor            
)

# LLM ëª¨ë¸ ì„ ì–¸
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# í˜„ì¬ ì‹œê°„ ì„ ì–¸
KST = ZoneInfo("Asia/Seoul")
today_str = dt.datetime.now(KST).strftime("%Y-%m-%d %H:%M:%S")

# ì§ˆë¬¸ í”„ë¡¬í”„íŠ¸ (ì§ˆë¬¸ ì¬êµ¬ì„±/ë§¥ë½í™”)
contextualize_q_system_prompt = """

ì´ì „ ëŒ€í™”ê°€ ìˆë‹¤ë©´ ì°¸ê³ í•˜ì—¬,
ì‚¬ìš©ìì˜ ìµœì‹  ì§ˆë¬¸ì„ ë…ë¦½ì ìœ¼ë¡œ ì´í•´ ê°€ëŠ¥í•œ í•œ ë¬¸ì¥ìœ¼ë¡œ ë°”ê¿”ì£¼ì„¸ìš”.
ë‹µë³€í•˜ì§€ ë§ê³  ì§ˆë¬¸ë§Œ ì¬ì‘ì„±í•˜ì„¸ìš”.

"""

contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)


history_aware_retriever = create_history_aware_retriever(
    llm,
    upgraded_retriever,             # Cohere API KEY ì—†ëŠ” ê²½ìš° >> reordered_retriever
    contextualize_q_prompt
)


# ë‹µë³€ í”„ë¡¬í”„íŠ¸
qa_system_prompt = """

ë‹¹ì‹ ì€ 'ëª¨ë‘ì˜ì—°êµ¬ì†Œ(ëª¨ë‘ì—°)' ìˆ˜ê°•ìƒë“¤ì˜ ë¹„ì„œì…ë‹ˆë‹¤.
í˜„ì¬ ì‹œê°„ì€ {today} (KST)ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ 'ì–´ì œ, ë‚´ì¼' ë“±ì˜ í‘œí˜„ì€ {today}ë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŒŒì•…í•˜ì„¸ìš”.
ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ ë‹µí•˜ì„¸ìš”. ê·¼ê±°ê°€ ì—†ìœ¼ë©´ 'ì •ë³´ê°€ ëª…í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìš´ì˜ë§¤ë‹ˆì €ë‹˜ì´ë‚˜ í¼ì‹¤ë‹˜ê»˜ ë¬¸ì˜í•´ì£¼ì„¸ìš”.'ë¼ê³ ë§Œ ëŒ€ë‹µí•˜ì„¸ìš”.
ì‚¬ìš©ì ì…ë ¥ì— í¬í•¨ëœ ì‚¬ì‹¤ì€ ê·¼ê±°ë¡œ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
í›ˆë ¨ì¥ë ¤ê¸ˆì˜ ê²½ìš° ì£¼ì–´ì§„ ë‹¨ìœ„ ê¸°ê°„ ì¼ìˆ˜ì˜ 80%ì´ìƒì„ ì¶œì„í•´ì•¼ë§Œ ê¸ˆì•¡ì´ ì§€ê¸‰ë¨ì„ ëª…ì‹¬í•˜ì„¸ìš”. 
ìµœëŒ€ 3ë¬¸ì¥ìœ¼ë¡œ ì§§ê²Œ ë‹µë³€í•˜ì„¸ìš”.

{context}"""

qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", qa_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

qa_prompt = qa_prompt.partial(today=today_str)

# RAG - CHAIN



question_answer_chain = create_stuff_documents_chain(llm, qa_prompt) # ê²€ìƒ‰ëœ ëª¨ë“  ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ê¸´ ë¬¸ìì—´ë¡œ ë­‰ì³ í”„ë¡¬í”„íŠ¸ì— ë„£ê³  llmì— ì „ë‹¬
rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain) # historyì™€ ì—°ê²°


# RAG - SESSION
#ì±„íŒ… ì„¸ì…˜ë³„ ê¸°ë¡ ì €ì¥ ìœ„í•œ Dictionary ì„ ì–¸
store = {}

#ì£¼ì–´ì§„ session_id ê°’ì— ë§¤ì¹­ë˜ëŠ” ì±„íŒ… íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ ì„ ì–¸
def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]


#RunnableWithMessageHistory ëª¨ë“ˆë¡œ rag_chainì— ì±„íŒ… ê¸°ë¡ ì„¸ì…˜ë³„ë¡œ ìë™ ì €ì¥ ê¸°ëŠ¥ ì¶”ê°€
conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
    output_messages_key="answer",
)

# ============================================== #

# Lottie ì• ë‹ˆë©”ì´ì…˜ ë¡œë”© í•¨ìˆ˜
def load_lottie_url(url):
    r = requests.get(url)
    if r.status_code != 200:
        return None
    return r.json()

# ëˆˆ ë‚´ë¦¬ëŠ” Lottie ì• ë‹ˆë©”ì´ì…˜ URL
snow_lottie_url = "https://assets2.lottiefiles.com/packages/lf20_1pxqjqps.json"
snow_animation = load_lottie_url(snow_lottie_url)

st_lottie(
    snow_animation,
    speed=1,
    reverse=False,
    loop=True,
    quality="high",
    height=500,
    width=800,
    key="snow"
)

rag_chain = conversational_rag_chain

html_code = """
<div style="text-align: center;">
    <p style="font-size:25px;">
        ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ëª¨ë‘ë´‡ì…ë‹ˆë‹¤.<br>ì¦ê±°ìš´ ëª¨ë‘ì—° ìƒí™œì„ ìœ„í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.ğŸ˜Š
    </p>
</div>
"""

# st.markdownì„ ì‚¬ìš©í•˜ì—¬ HTMLì„ ì‚½ì…í•©ë‹ˆë‹¤.
st.markdown(html_code, unsafe_allow_html=True)

if "session_id" not in st.session_state:
    st.session_state["session_id"] = "default"
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"}]

for msg in st.session_state["messages"]:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt_message := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš” :)"):
    st.session_state["messages"].append({"role": "user", "content": prompt_message})
    st.chat_message("user").write(prompt_message)

    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            # conversational_rag_chainì€ ìƒë‹¨ì—ì„œ ì •ì˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •
            resp = conversational_rag_chain.invoke(
                {"input": prompt_message},
                config={"configurable": {"session_id": st.session_state["session_id"]}},
            )
            answer = resp if isinstance(resp, str) else resp.get("answer", "")
            st.write(answer)
            st.session_state["messages"].append({"role": "assistant", "content": answer})


with st.sidebar:
    
    st.title("ğŸ€ëª¨ë‘ì˜ ì—°êµ¬ì†Œ")

    # ê´€ë ¨ ì‚¬ì´íŠ¸
    st.markdown("---") # êµ¬ë¶„ì„  ì¶”ê°€
    st.header("ğŸ”—ê´€ë ¨ ì‚¬ì´íŠ¸")
    st.link_button(
        label="ëª¨ë‘ì˜ ì—°êµ¬ì†Œ í™ˆí˜ì´ì§€", # ë²„íŠ¼ì— í‘œì‹œë  í…ìŠ¤íŠ¸
        url="https://modulabs.co.kr",    # ì´ë™í•  URL     
    )
    st.link_button(
        label="ë°ì‹¸ 5ê¸° ë…¸ì…˜ ì›Œí¬ìŠ¤í˜ì´ìŠ¤", # ë²„íŠ¼ì— í‘œì‹œë  í…ìŠ¤íŠ¸
        url="https://www.notion.so/New-5-25-07-07-26-01-08-New-23f2d25db62480828becc399aaa41877",    # ì´ë™í•  URL     
    )
    st.link_button(
        label="ë°ì‹¸ 5ê¸° ZEP", # ë²„íŠ¼ì— í‘œì‹œë  í…ìŠ¤íŠ¸
        url="https://zep.us/play/8l5Vdo",    # ì´ë™í•  URL     
    )
    st.link_button(
        label="LMS í™ˆí˜ì´ì§€", # ë²„íŠ¼ì— í‘œì‹œë  í…ìŠ¤íŠ¸
        url="https://lms.aiffel.io/",    # ì´ë™í•  URL     
    )

    # ì²¨ë¶€íŒŒì¼
    st.markdown("---") # êµ¬ë¶„ì„  ì¶”ê°€
    st.header("ğŸ“„ì²¨ë¶€íŒŒì¼")
    with open('../data/íœ´ê°€ì‹ ì²­ì„œ(ë°ì‹¸_5ê¸°).docx', 'rb') as file:
        st.download_button(
            label='íœ´ê°€ì‹ ì²­ì„œ ë‹¤ìš´ë¡œë“œ',
            data=file,
            file_name='íœ´ê°€ì‹ ì²­ì„œ.docx',
            mime='application/vnd.openxmlformats-officedocument.wordprocessingml.document'
        )

    # ë‚ ì§œ ì„ íƒ (dt ë³„ì¹­ ì‚¬ìš©)
    st.markdown("---")
    st.header("ğŸ—“ï¸ ë‚ ì§œ ì„ íƒ")
    today = dt.date.today()  # â† ì—¬ê¸°!
    selected_date = st.date_input(
        "ì›í•˜ëŠ” ë‚ ì§œë¥¼ ì„ íƒí•˜ì„¸ìš”:",
        value=today,
        min_value=dt.date(2023, 1, 1),
        max_value=dt.date(2026, 12, 31),
        key="sidebar_date"
    )
    st.markdown("---")
    st.info(f"ì˜¤ëŠ˜ì€: **{selected_date}ì…ë‹ˆë‹¤.**")