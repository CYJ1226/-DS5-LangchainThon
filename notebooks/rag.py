import os
import streamlit as st
from dotenv import load_dotenv
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
import pandas as pd
import plotly.express as px
from streamlit_lottie import st_lottie
import requests
import datetime as dt

#ì˜¤í”ˆAI API í‚¤ ì„¤ì •
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
COHERE_API_KEY = os.getenv("COHERE_API_KEY")


# ============================================== #

# Embedding - Chunk split
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=128,
    separators=["\n\n", "\n", " ", ""]             
)


# pdf 
from langchain_community.document_loaders import PyPDFLoader

PDF_PATH = r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\pdf\ëª¨ë‘ì—° ë¸Œëœë”©ë¶ ì •ë¦¬.pdf"

loader_pdf = PyPDFLoader(PDF_PATH)
pages_pdf = loader_pdf.load()

for d in pages_pdf:
    d.metadata["source_type"] = "pdf"
    d.metadata["source"] = os.path.basename(PDF_PATH)

docs_pdf = text_splitter.split_documents(pages_pdf)


# html
from langchain_community.document_loaders import UnstructuredHTMLLoader

HTML_PATH = [
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\html\LMS oops í•´ê²°ë²•.html",
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\html\LMS ì•„ì´í  ë…¸íŠ¸ë¶ì´ ì•„ë‹™ë‹ˆë‹¤ ì—ëŸ¬.html",
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\html\LMS ì´ìš©ì‹œ ë°œìƒí•˜ëŠ” ë¬¸ì œ í•´ê²°ë²•.html",
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\html\êµìœ¡ê³¼ì • ì¤‘ ì·¨ì—… ì‹œ.html",
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\html\ë°ì‹¸ 5ê¸° í›ˆë ¨ ì •ë³´.html",
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\html\ìˆ˜ê°• ì¤‘ ê³ ìš© í˜•íƒœ ê´€ë ¨ ì•ˆë‚´.html",
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\html\ìŠ¤í„°ë””ë¥¼ ë§Œë“¤ê³  ì‹¶ì€ë° ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”.html",
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\html\ì˜¤í”„ë‹ ì¥ì†Œì™€ í´ë¡œì§• ì¥ì†Œê°€ ë‹¤ë¦…ë‹ˆë‹¤.html",
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\html\ì¸í„°ë„·ì´ ë¶ˆì•ˆì •í•˜ì—¬ ì¶œê²° QRì„ ì œëŒ€ë¡œ ì°ì§€ ëª»í•˜ì˜€ìŠµë‹ˆë‹¤.html",
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\html\ì œì  ê°€ì´ë“œ.html",
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\html\ì¶œê²° ë° ê³µê°€ì— ëŒ€í•˜ì—¬.html",
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\html\íˆ´ ì„¸íŒ….html",
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\html\í›ˆë ¨ ì¥ë ¤ê¸ˆ ì§€ê¸‰ í™•ì¸.html",
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\html\í›ˆë ¨ ì°¸ì—¬ ê·œì¹™.html",
]

html_list = []

# ê° íŒŒì¼ ë¡œë“œ + Metadata ì €ì¥
for path in HTML_PATH:
    loader_html = UnstructuredHTMLLoader(path)
    pages_html = loader_html.load()

    for d in pages_html:
        d.metadata["source_type"] = "html"
        d.metadata["source"] = os.path.basename(path)

    html_list.extend(pages_html)  

docs_html = text_splitter.split_documents(html_list)


# word
from langchain_community.document_loaders import Docx2txtLoader

WORD_PATH = r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\word\íœ´ê°€ì‹ ì²­ì„œ(ë°ì‹¸_5ê¸°).docx"
loader_word = Docx2txtLoader(WORD_PATH)
pages_word = loader_word.load()

for d in pages_word:
    d.metadata["source_type"] = "word"
    d.metadata["source"] = os.path.basename(WORD_PATH)

docs_word = text_splitter.split_documents(pages_word)


# csv
from langchain_community.document_loaders.csv_loader import CSVLoader

CSV_PATH = [
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\csv\ë°ì‹¸ 5ê¸° ë™ë£Œë“¤.csv",
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\csv\ë°ì‹¸ 5ê¸° ìš´ì˜ì§„.csv"
]

csv_list = []

# ê° íŒŒì¼ ë¡œë“œ
for path in CSV_PATH:
    loader_csv = CSVLoader(path, encoding = 'cp949')
    pages_csv = loader_csv.load()

    for d in pages_csv:
        d.metadata["source_type"] = "csv"
        d.metadata["source"] = os.path.basename(path)

    csv_list.extend(pages_csv)  

docs_csv = text_splitter.split_documents(csv_list)


# csv - calendar

from langchain_core.documents import Document
import pandas as pd

# í•™ìƒ ì´ë¦„ì„ ê¸°ì¤€ ê·¸ë£¹í™” í•¨ìˆ˜
def create_grouped_documents(csv_path: str) -> list[Document]:
    """
    CSV íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ í•™ìƒ ì´ë¦„ë³„ë¡œ ì¶œê²° ê¸°ë¡ì„ ê·¸ë£¹í™”í•˜ê³ 
    LangChain Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        csv_path: ì¶œê²° CSV íŒŒì¼ì˜ ê²½ë¡œ.

    Returns:
        Document ê°ì²´ ë¦¬ìŠ¤íŠ¸. ê° DocumentëŠ” í•œ í•™ìƒì˜ ì „ì²´ ê¸°ë¡ì„ ë‹´ìŠµë‹ˆë‹¤.
    """
    # 1. CSV íŒŒì¼ ë¡œë“œ
    df = pd.read_csv(csv_path, encoding='cp949')

    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒí•˜ê³  NaN ê°’ì€ ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´ (ë¬¸ìì—´ ê²°í•© ì‹œ ì˜¤ë¥˜ ë°©ì§€)
    required_cols = ['ì´ë¦„', 'ì‚¬ìœ ', 'ë‚ ì§œ', 'ë¶€ì¬ì‹œê°„', 'ìƒíƒœ']
    if not all(col in df.columns for col in required_cols):
        print(f"ì˜¤ë¥˜: CSV íŒŒì¼ì— í•„ìš”í•œ ì»¬ëŸ¼ ({required_cols}) ì¤‘ ì¼ë¶€ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return []

    df = df[required_cols].fillna('')

    # Document ê°ì²´ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    documents = []

    # 2. 'ì´ë¦„' ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”
    grouped = df.groupby('ì´ë¦„')

    # 3. ê° í•™ìƒ ê·¸ë£¹ì„ í•˜ë‚˜ì˜ ê¸´ í…ìŠ¤íŠ¸ Documentë¡œ ë³€í™˜
    for name, group_df in grouped:
        # í•™ìƒë³„ ê¸°ë¡ì„ ë¬¸ìì—´ë¡œ ë³€í™˜ (ë‚ ì§œ, ë¶€ì¬ì‹œê°„, ìƒíƒœë§Œ í¬í•¨)
        # 'ì´ë¦„' ì»¬ëŸ¼ì€ ë©”íƒ€ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— í…ìŠ¤íŠ¸ ë‚´ìš©ì—ì„œëŠ” ì œì™¸í•©ë‹ˆë‹¤.
        record_strings = []
        for index, row in group_df.iterrows():
            record = (
                f"ì‚¬ìœ : {row['ì‚¬ìœ ']}, "
                f"ë‚ ì§œ: {row['ë‚ ì§œ']}, "
                f"ìƒíƒœ: {row['ìƒíƒœ']}, "
                f"ë¶€ì¬ì‹œê°„: {row['ë¶€ì¬ì‹œê°„']}"
            )
            record_strings.append(record)

        # ëª¨ë“  ê¸°ë¡ì„ ì¤„ ë°”ê¿ˆìœ¼ë¡œ ì—°ê²°í•˜ì—¬ í•˜ë‚˜ì˜ ê¸´ í…ìŠ¤íŠ¸ ìƒì„±
        full_records_text = "\n".join(record_strings)

        # ìµœì¢… Document ê°ì²´ ìƒì„±
        document = Document(
            page_content=(
                f"í•™ìƒ ì´ë¦„: {name}\n\n"
                f"--- ì „ì²´ ì¶œê²° ê¸°ë¡ ì‹œì‘ ---\n"
                f"{full_records_text}"
            ),
            # ë©”íƒ€ë°ì´í„°ì— í•µì‹¬ ì •ë³´ ì €ì¥ (ê²€ìƒ‰ ì‹œ í™œìš© ê°€ëŠ¥)
            metadata={'í•™ìƒì´ë¦„': name, 'ì´ê¸°ë¡ìˆ˜': len(group_df)}
        )
        documents.append(document)

    return documents

# í•™ìƒë³„ Document ë¦¬ìŠ¤íŠ¸ ìƒì„±
attendance_documents = create_grouped_documents(csv_path=r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\csv\ë°ì‹¸ 5ê¸° ì¼ì •í‘œ.csv")

docs_attendance = text_splitter.split_documents(attendance_documents)



# VECTOR DB
from langchain.retrievers import ContextualCompressionRetriever
from langchain_cohere import CohereRerank
from langchain_community.document_transformers import LongContextReorder
from langchain.retrievers.document_compressors import DocumentCompressorPipeline

# ChromaDB ë²¡í„° ì„ë² ë”© í›„ ì €ì¥
vectorstore = Chroma.from_documents(docs_html, OpenAIEmbeddings(model='text-embedding-3-large'))

vectorstore.add_documents(docs_word)
vectorstore.add_documents(docs_csv)
vectorstore.add_documents(docs_pdf)
vectorstore.add_documents(docs_attendance)



# RAG

from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import AIMessage, HumanMessage
from langchain.retrievers.document_compressors import LLMChainExtractor
from zoneinfo import ZoneInfo

# Reranking ì´ì „ base 
base_retriever = vectorstore.as_retriever(
    search_type="mmr", 
    search_kwargs={"lambda_mult": 0.4, "fetch_k": 96, "k": 48}
)

# Rerank
reranker = CohereRerank(
    model="rerank-multilingual-v3.0",    
    top_n=10                              
)

# Reorder
reorder = LongContextReorder()

# Rerank + Reorder
compressor = DocumentCompressorPipeline(transformers=[reranker, reorder])

upgraded_retriever = ContextualCompressionRetriever(
    base_retriever=base_retriever,
    base_compressor=compressor            
)


# LLM ëª¨ë¸ ì„ ì–¸
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# í˜„ì¬ ì‹œê°„ ì„ ì–¸
KST = ZoneInfo("Asia/Seoul")
today_str = dt.datetime.now(KST).strftime("%Y-%m-%d %H:%M:%S")

# ì§ˆë¬¸ í”„ë¡¬í”„íŠ¸
contextualize_q_system_prompt = """

ì´ì „ ëŒ€í™”ê°€ ìˆë‹¤ë©´ ì°¸ê³ í•˜ì—¬,
ì‚¬ìš©ìì˜ ìµœì‹  ì§ˆë¬¸ì„ ë…ë¦½ì ìœ¼ë¡œ ì´í•´ ê°€ëŠ¥í•œ í•œ ë¬¸ì¥ìœ¼ë¡œ ë°”ê¿”ì£¼ì„¸ìš”.
ë‹µë³€í•˜ì§€ ë§ê³  ì§ˆë¬¸ë§Œ ì¬ì‘ì„±í•˜ì„¸ìš”.

"""

contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)


history_aware_retriever = create_history_aware_retriever(
    llm,
    upgraded_retriever,             # Cohere API KEY ì—†ëŠ” ê²½ìš° >> reordered_retriever
    contextualize_q_prompt
)


# ë‹µë³€ í”„ë¡¬í”„íŠ¸
qa_system_prompt = """

ë‹¹ì‹ ì€ 'ëª¨ë‘ì˜ì—°êµ¬ì†Œ(ëª¨ë‘ì—°)' ìˆ˜ê°•ìƒë“¤ì˜ ë¹„ì„œì…ë‹ˆë‹¤.
í˜„ì¬ ì‹œê°„ì€ {today} (KST)ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ 'ì–´ì œ, ë‚´ì¼' ë“±ì˜ í‘œí˜„ì€ {today}ë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŒŒì•…í•˜ì„¸ìš”.
ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ ë‹µí•˜ì„¸ìš”. ê·¼ê±°ê°€ ì—†ìœ¼ë©´ 'ì •ë³´ê°€ ëª…í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìš´ì˜ë§¤ë‹ˆì €ë‹˜ì´ë‚˜ í¼ì‹¤ë‹˜ê»˜ ë¬¸ì˜í•´ì£¼ì„¸ìš”.'ë¼ê³ ë§Œ ëŒ€ë‹µí•˜ì„¸ìš”.
ì‚¬ìš©ì ì…ë ¥ì— í¬í•¨ëœ ì‚¬ì‹¤ì€ ê·¼ê±°ë¡œ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
ìµœëŒ€ 3ë¬¸ì¥ìœ¼ë¡œ ì§§ê²Œ ë‹µë³€í•˜ì„¸ìš”.

{context}"""
qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", qa_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

qa_prompt = qa_prompt.partial(today=today_str)

# RAG - CHAIN

from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain


question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)


# RAG - SESSION

from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

#ì±„íŒ… ì„¸ì…˜ë³„ ê¸°ë¡ ì €ì¥ ìœ„í•œ Dictionary ì„ ì–¸
store = {}

#ì£¼ì–´ì§„ session_id ê°’ì— ë§¤ì¹­ë˜ëŠ” ì±„íŒ… íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ ì„ ì–¸
def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]


#RunnableWithMessageHistory ëª¨ë“ˆë¡œ rag_chainì— ì±„íŒ… ê¸°ë¡ ì„¸ì…˜ë³„ë¡œ ìë™ ì €ì¥ ê¸°ëŠ¥ ì¶”ê°€
conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
    output_messages_key="answer",
)

# ============================================== #


# Streamlit UI(ì—¬ê¸°ë¶€í„° ìŠ¤íŠ¸ë¦¼ë¦¿ ì½”ë“œì´ë‹ˆ ì ìš©í•  ë¶€ë¶„)
# st.markdown(
#     "<div style='text-align: center; color: #CD5C5C; font-size: 100px;'>ëª¨ë‘ë´‡</div>",
#     unsafe_allow_html=True
# )

# rag_chain = chaining()
# st.snow()


st.set_page_config(page_title="ëª¨ë‘ë´‡", layout="centered")

# Lottie ì• ë‹ˆë©”ì´ì…˜ ë¡œë”© í•¨ìˆ˜
def load_lottie_url(url):
    r = requests.get(url)
    if r.status_code != 200:
        return None
    return r.json()

# ëˆˆ ë‚´ë¦¬ëŠ” Lottie ì• ë‹ˆë©”ì´ì…˜ URL
snow_lottie_url = "https://assets2.lottiefiles.com/packages/lf20_1pxqjqps.json"
snow_animation = load_lottie_url(snow_lottie_url)

# Streamlit ì•± êµ¬ì„±
# st.set_page_config(page_title="ì¸ì‚¬í•˜ëŠ” ë¡œë´‡", layout="centered")
st.markdown(
    "<div style='text-align: center; color: #CD5C5C; font-size: 100px;'>ëª¨ë‘ë´‡</div>",
    unsafe_allow_html=True
)
st.title("â„ï¸ ì•ˆë…•í•˜ì„¸ìš”!! ëª¨ë‘ë´‡ì…ë‹ˆë‹¤.")

# st.write("AIë¡œë´‡ì´ ì¸ì‚¬ë¥¼ í•©ë‹ˆë‹¤.")

# ì• ë‹ˆë©”ì´ì…˜ í‘œì‹œ
# col1, col2, col3 = st.columns([1, 2, 1])
# with col2:
st_lottie(
    snow_animation,
    speed=1,
    reverse=False,
    loop=True,
    quality="high",
    height=500,
    width=800,
    key="snow"
)

st.snow()
# st.title("ModuBot")
# st.header("ğŸ’¬ ì—¬ëŸ¬ë¶„ì˜ ëª¨ë‘ë´‡ì…ë‹ˆë‹¤ ğŸ“š")
rag_chain = conversational_rag_chain
# st.snow()

# st.data_input
st.markdown("---")
st.write("ì¦ê±°ìš´ ëª¨ë‘ì—° ìƒí™œì„ ìœ„í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.ğŸ˜Š")

if "session_id" not in st.session_state:
    st.session_state["session_id"] = "default"
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"}]

for msg in st.session_state["messages"]:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt_message := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš” :)"):
    st.session_state["messages"].append({"role": "user", "content": prompt_message})
    st.chat_message("user").write(prompt_message)

    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            # conversational_rag_chainì€ ìƒë‹¨ì—ì„œ ì •ì˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •
            resp = conversational_rag_chain.invoke(
                {"input": prompt_message},
                config={"configurable": {"session_id": st.session_state["session_id"]}},
            )
            answer = resp if isinstance(resp, str) else resp.get("answer", "")
            st.write(answer)
            st.session_state["messages"].append({"role": "assistant", "content": answer})


with st.sidebar:
    st.title("âš™ï¸ëª¨ë‘ì˜ ì—°êµ¬ì†Œ")
    st.markdown("---") # êµ¬ë¶„ì„  ì¶”ê°€
    
    menu = st.sidebar.selectbox("ì›í•˜ì‹œëŠ” ì •ë³´ë¥¼ ì„ íƒí•˜ì„¸ìš”", ["ëª¨ë‘ì—°ìƒí™œ", "ì‰ë°¸ê·¸íˆ¬", "í•™ìŠµ"])

# ì„ íƒëœ ë©”ë‰´ì— ë”°ë¼ ë‹¤ë¥¸ ì½˜í…ì¸  ì¶œë ¥
    if menu == "ëª¨ë‘ì—°ìƒí™œ":
        st.header("ğŸ“˜ ëª¨ë‘ì—°ìƒí™œ")
        st.write("ì½˜í…ì¸ ë¥¼ ì—¬ê¸°ì— í‘œì‹œí•©ë‹ˆë‹¤.")
    elif menu == "ì‰ë²¨ê·¸íˆ¬":
        st.header("ğŸ“— ì‰ë°¸ê·¸íˆ¬")
        st.write("ì½˜í…ì¸ ë¥¼ ì—¬ê¸°ì— í‘œì‹œí•©ë‹ˆë‹¤.")
    elif menu == "í•™ìŠµ":
        st.header("ğŸ“™ í•™ìŠµ")
        st.write("ì½˜í…ì¸ ë¥¼ ì—¬ê¸°ì— í‘œì‹œí•©ë‹ˆë‹¤.")
    elif menu == "ì¶œê²°ê´€ë¦¬":
        st.header("ğŸ“™ ì¶œê²°")
        st.write("ì½˜í…ì¸ ë¥¼ ì—¬ê¸°ì— í‘œì‹œí•©ë‹ˆë‹¤.")

# menu = st.sidebar.selectbox("íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”", ["ëª¨ë‘ì—°ìƒí™œ", "ì‰ë²¨ê·¸íˆ¬", "í•™ìŠµ", 'ì¶œê²°ê´€ë¦¬'])

# # ê³¼ëª©ë³„ PDF íŒŒì¼ ê²½ë¡œ ì„¤ì • (ì˜ˆì‹œ: ë¡œì»¬ ë˜ëŠ” URL)
# pdf_files = {
#     "ëª¨ë‘ì—°ìƒí™œ": "data/korean.pdf",
#     "ì‰ë²¨ê·¸íˆ¬": "data/english.pdf",
#     "í•™ìŠµ": "data/math.pdf",
#     "ì¶œê²°ê´€ë¦¬": "data/math.pdf"
# }

# # ì„ íƒëœ ê³¼ëª©ì— ë”°ë¼ PDF í‘œì‹œ
# st.header(f"ğŸ“˜ {menu} ë¥¼ ìœ„í•´ ì›í•˜ëŠ” ìë£Œ")

# pdf_path = pdf_files.get(menu)

# try:
#     with open(pdf_path, "rb") as f:
#         pdf_bytes = f.read()
#         st.download_button(
#             label=f"{menu} PDF ë‹¤ìš´ë¡œë“œ",
#             data=pdf_bytes,
#             file_name=f"{menu}.pdf",
#             mime="application/pdf"
#         )
#         st.write("PDF íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ í™•ì¸í•˜ì„¸ìš”.")
# except FileNotFoundError:
#     st.warning("PDF íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

# PDF ë‹¤ìš´ë¡œë“œ
    pdf_files = {
        "ëª¨ë‘ì—°ìƒí™œ": "data/korean.pdf",
        "ì‰ë°¸ê·¸íˆ¬": "data/english.pdf",
        "í•™ìŠµ": "data/math.pdf",
        "ì¶œê²°ê´€ë¦¬": "data/math.pdf"
    }

    selected_file = st.selectbox("íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”", list(pdf_files.keys()))
    pdf_path = pdf_files.get(selected_file)

    try:
        with open(pdf_path, "rb") as f:
            pdf_bytes = f.read()
            st.download_button(
                label=f"{selected_file} PDF ë‹¤ìš´ë¡œë“œ",
                data=pdf_bytes,
                file_name=f"{selected_file}.pdf",
                mime="application/pdf"
            )
            st.write("PDF íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ í™•ì¸í•˜ì„¸ìš”.")
    except FileNotFoundError:
        st.warning("PDF íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

# ë§í¬
# st.sidebar.markdown("[ëª¨ë‘ì—° í™ˆ](https://https://modulabs.co.kr)")
    st.markdown("[ëª¨ë‘ì—° í™ˆ](https://modulabs.co.kr)")
    st.markdown("<a href='https://biz.modulabs.co.kr/event' target='_blank'>í˜ì´ì§€ ì—´ê¸°</a>", unsafe_allow_html=True)


# ë‚ ì§œ ì„ íƒ (dt ë³„ì¹­ ì‚¬ìš©)
    st.header("ğŸ—“ï¸ ë‚ ì§œ ì„ íƒ")
    st.markdown("---")
    today = dt.date.today()  # â† ì—¬ê¸°!
    selected_date = st.date_input(
        "ì›í•˜ëŠ” ë‚ ì§œë¥¼ ì„ íƒí•˜ì„¸ìš”:",
        value=today,
        min_value=dt.date(2023, 1, 1),
        max_value=dt.date(2026, 12, 31),
        key="sidebar_date"
        )
    st.markdown("---")
    st.info(f"ì˜¤ëŠ˜ì€: **{selected_date}ì…ë‹ˆë‹¤.**")




# # ë˜ëŠ” HTMLë¡œ ìƒˆ ì°½ ì—´ê¸°
# st.sidebar.markdown(
#     "<a href='https://biz.modulabs.co.kr/event' target='_blank'> í˜ì´ì§€ ì—´ê¸°</a>",
#     unsafe_allow_html=True
# )


# # --- í˜ì´ì§€ ì„¤ì • ---
# st.set_page_config(
#     page_title="ì‚¬ì´ë“œë°” ë‹¬ë ¥ ì˜ˆì œ",
#     layout="wide" # ë„“ì€ ë ˆì´ì•„ì›ƒìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ì‚¬ì´ë“œë°”ì™€ ë©”ì¸ ì»¨í…ì¸  ì˜ì—­ì„ í™•ë³´
# )

# # --- ì‚¬ì´ë“œë°” ë‹¬ë ¥ í‘œì‹œ ---

# # 1. ì‚¬ì´ë“œë°” ì‹œì‘
# st.sidebar.header("ğŸ—“ï¸ ë‚ ì§œ ì„ íƒ")
# st.sidebar.markdown("---")

# # 2. st.date_inputì„ ì‚¬ìš©í•˜ì—¬ ë‚ ì§œ ì…ë ¥ ìœ„ì ¯ (ë‹¬ë ¥) í‘œì‹œ
# # ê¸°ë³¸ê°’ìœ¼ë¡œ ì˜¤ëŠ˜ ë‚ ì§œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
# today = datetime.date.today()
# selected_date = st.sidebar.date_input(
#     label="ì›í•˜ëŠ” ë‚ ì§œë¥¼ ì„ íƒí•˜ì„¸ìš”:",
#     value=today,
#     min_value=datetime.date(2023, 1, 1), # ìµœì†Œ ë‚ ì§œ ì„¤ì •
#     max_value=datetime.date(2026, 12, 31) # ìµœëŒ€ ë‚ ì§œ ì„¤ì •
# )

# st.sidebar.markdown("---")
# st.sidebar.info(f"ì˜¤ëŠ˜ì€: **{selected_date}ì…ë‹ˆë‹¤.**")

# # --- ë©”ì¸ ì»¨í…ì¸  ì˜ì—­ ---
# # st.title("Streamlit ì‚¬ì´ë“œë°” ë‹¬ë ¥ ì˜ˆì œ")
# # st.write(f"ì‚¬ì´ë“œë°”ì—ì„œ ì„ íƒí•˜ì‹  ë‚ ì§œëŠ” **{selected_date}** ì…ë‹ˆë‹¤.")
# # st.write("ì´ ë‚ ì§œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë©”ì¸ ì•±ì˜ ë‚´ìš©ì„ ë™ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")



 # ì¶œì„ í˜„í™© ê·¸ë˜í”„ ì˜ˆì‹œ     ì‘ì—…í•œ ë‚´ìš©ê³¼ ì—°ê³„ê°€ ê°€ëŠ¥í•˜ë‹¤ë©´ êµ¬í˜„ì´ ë ë“¯   ì•ˆë˜ë©´ ì‚­ì œí•  ë¶€ë¶„

# data = {
#     "ì´ë¦„": ["ê¹€ë¯¼ìˆ˜", "ì´ì˜í¬", "ë°•ì² ìˆ˜", "ìµœì§€ìš°", "ì •ìš°ì„±"],
#     "ì¶œì„ íšŸìˆ˜": [18, 20, 15, 19, 17]
# }

# df = pd.DataFrame(data)

# # ì œëª©
# st.title("ğŸ“Š ì¶œì„ í˜„í™©")

# # ë°ì´í„° í…Œì´ë¸” í‘œì‹œ
# # st.subheader("ì¶œì„ ë°ì´í„°")
# # st.dataframe(df)

# # ë§‰ëŒ€ ê·¸ë˜í”„ ì‹œê°í™”
# fig = px.bar(df, x="ì´ë¦„", y="ì¶œì„ íšŸìˆ˜", color="ì´ë¦„",
#              title="í•™ìƒë³„ ì¶œì„ ê²°ê³¼",
#              labels={"ì´ë¦„": "í•™ìƒ ì´ë¦„", "ì¶œì„ íšŸìˆ˜": "ì¶œì„ ìˆ˜"},
#              height=400)

# st.plotly_chart(fig)



